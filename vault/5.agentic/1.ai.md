---
title: MCP Server
description: Connect your documentation to AI tools with a native MCP server.
order: 1
icon: i-lucide-cpu
navigation:
  icon: i-lucide-cpu
aliases:
  - Model Context Protocol
  - AI Integration
tags:
  - ai
  - mcp
  - agents
---

In the age of generative AI, documentation is no longer just for humans—it's for agents too. Lithos includes a native **Model Context Protocol (MCP)** server, making your entire knowledge base instantly accessible to AI assistants like Claude, Cursor, and Visual Studio Code.

## Native Model Intelligence

Lithos treats AI integration as a first-class citizen, not an afterthought.

### The MCP Standard
The [Model Context Protocol](https://modelcontextprotocol.io/) is an open standard that defines how AI models interact with external data. By adopting this standard, Lithos ensures that your documentation is not just a static website, but a dynamic **Context Server**. This means any MCP-compliant AI client can "plug in" to your documentation and read it in real-time.

### Zero-Configuration
For most users, there is absolutely nothing to configure. The MCP server starts automatically alongside your Docus dev server.
- **Production Ready**: When deployed, the MCP endpoints are served alongside your static assets.
- **Auto-Discovery**: Clients can query the server capabilities without manual mapping.
- **Seamless Updates**: As you update your markdown files, the AI context is updated instantly. 

### Privacy & Control
Since Lithos processes your Obsidian vault locally, your interaction with local AI models remains private. You decide what is exposed. The API is read-only by default, ensuring that AI agents can learn from your docs but cannot hallucinate changes or delete your managed content.

## Semantic Integration

We don't just dump text into the context window; we structure it for semantic understanding.

### Proactive Tool Use
When connected, the AI doesn't just "know" your docs exists; it has tools to explore it.
- **`list_pages`**: The Agent can scan your directory structure to understand the scope of available topics.
- **`get_page`**: The Agent can fetch specific content when it identifies a relevant file.
- **`search_docs`**: The Agent can perform semantic searches to find answers to user questions.
This "active retrieval" pattern is far superior to simply pasting content into a chat window, as the AI decides *what* it needs based on the conversation flow.

### `llms.txt` Generation
For lighter-weight integrations or web crawling, Lithos automatically generates a standard `/llms.txt` file at the root of your site. This file provides a curated, token-optimized summary of your documentation, designed specifically for training data or quick context ingestion by simpler models that don't support full MCP.

> [!NOTE] Native Support
> Lithos exposes your vault's `llms.txt` and full content via a standard MCP API endpoint.

### Context Awareness
Because Lithos understands the structure of your vault (links, hierarchy, tags), it preserves this metadata in the API context. When an AI reads a page, it also sees the *relationships*—what links here, what tags are used—allowing for much deeper reasoning. An AI can follow a trail of wikilinks just like a human reader would, hopping from concept to concept to build a complete answer.

## Ecosystem Compatibility

Lithos is designed to play well with the tools you already use.

### Major Clients Supported
We test against the most popular AI coding environments:
- **Claude Desktop**: Connects directly via the MCP settings for full context in chat.
- **Cursor**: Use your docs as a `@doc` source for codebase indexing and RAG.
- **Visual Studio Code**: Integrate with Copilot Chat or other agents to query your docs while you code.
- **Windsurf & Zed**: Fully supported via their respective MCP configuration panels.

### Configuration Guide
Connecting to your local Lithos instance is usually as simple as adding a JSON configuration.

```json
{
  "mcpServers": {
    "my-docs": {
      "type": "http",
      "url": "http://localhost:3000/mcp"
    }
  }
}
```

This simple config tells your editor: "There is an intelligent documentation server running here."

### Extending the Server
For developers, the built-in server is extensible. Using the underlying runtime, you can add custom tools. Imagine a tool that not only reads your docs but `executes` code snippets from them, or checks the status of a project defined in your frontmatter. The possibilities for "Executable Documentation" are built right into the architecture.
